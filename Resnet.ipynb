{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IooH_0ijrQou"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (if dataset is stored there)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Paths to your dataset (adjust according to your dataset path in Google Drive)\n",
        "train_dir = '/content/drive/MyDrive/train'\n",
        "test_dir = '/content/drive/MyDrive/test'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5XK9a9nrYf1",
        "outputId": "0ca5d170-7601-4638-8815-5955c060e012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Image data preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Loading the images from the directories\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),  # ResNet50 expects 224x224 input images\n",
        "    batch_size=32,\n",
        "    class_mode='binary'  # Assuming binary classification (covid vs normal)\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4l6tvMxVrkdG",
        "outputId": "19ba2987-53dc-4683-d8e5-23200aaf1a9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2754 images belonging to 2 classes.\n",
            "Found 1288 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the pre-trained ResNet50 model without the top classification layer\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freezing the convolutional base\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Adding custom classification layers on top of ResNet50\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1, activation='sigmoid')(x)  # Output layer for binary classification\n",
        "\n",
        "# Final model\n",
        "model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Calculate steps_per_epoch and validation_steps based on dataset size\n",
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "validation_steps = test_generator.samples // test_generator.batch_size\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=10,\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=validation_steps\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save('/content/resnet50_covid19_classifier.h5')\n",
        "\n",
        "# Optionally: evaluate the model\n",
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCSH19XpsHQd",
        "outputId": "76d2ce65-558a-4335-c974-43d51921a7c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1056s\u001b[0m 12s/step - accuracy: 0.7837 - loss: 1.4388 - val_accuracy: 0.9102 - val_loss: 0.2985\n",
            "Epoch 2/10\n",
            "\u001b[1m 1/86\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 113ms/step - accuracy: 0.9375 - loss: 0.2932"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 96ms/step - accuracy: 0.9375 - loss: 0.2932 - val_accuracy: 0.8750 - val_loss: 0.2980\n",
            "Epoch 3/10\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 1s/step - accuracy: 0.8424 - loss: 0.4609 - val_accuracy: 0.9102 - val_loss: 0.3528\n",
            "Epoch 4/10\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7812 - loss: 0.4409 - val_accuracy: 0.8750 - val_loss: 0.3738\n",
            "Epoch 5/10\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 1s/step - accuracy: 0.8429 - loss: 0.4469 - val_accuracy: 0.9102 - val_loss: 0.2760\n",
            "Epoch 6/10\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.8125 - loss: 0.4383 - val_accuracy: 0.8750 - val_loss: 0.3135\n",
            "Epoch 7/10\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.8576 - loss: 0.4184 - val_accuracy: 0.9094 - val_loss: 0.2791\n",
            "Epoch 8/10\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - accuracy: 0.7812 - loss: 0.5573 - val_accuracy: 1.0000 - val_loss: 0.1970\n",
            "Epoch 9/10\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.8535 - loss: 0.4374 - val_accuracy: 0.9102 - val_loss: 0.2410\n",
            "Epoch 10/10\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step - accuracy: 0.9688 - loss: 0.2328 - val_accuracy: 0.8750 - val_loss: 0.3126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 446ms/step - accuracy: 0.9141 - loss: 0.2312\n",
            "Test Accuracy: 90.99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freezing the convolutional base\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Adding custom classification layers on top of ResNet50 with Leaky ReLU\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(256)(x)\n",
        "x = LeakyReLU(alpha=0.01)(x)  # Leaky ReLU activation with alpha parameter\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1, activation='sigmoid')(x)  # Output layer for binary classification\n",
        "\n",
        "# Final model\n",
        "model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "# Compile the model using Adam as the optimizer\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Calculate steps_per_epoch and validation_steps based on dataset size\n",
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "validation_steps = test_generator.samples // test_generator.batch_size\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=10,\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=validation_steps\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save('/content/resnet50_covid19_classifier.h5')\n",
        "\n",
        "# Optionally: evaluate the model\n",
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JLaCVQoW26o",
        "outputId": "3c2664ba-83ac-4641-811d-4b353d748d80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1523s\u001b[0m 17s/step - accuracy: 0.7605 - loss: 3.7906 - val_accuracy: 0.9094 - val_loss: 0.2969\n",
            "Epoch 2/10\n",
            "\u001b[1m 1/86\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 99ms/step - accuracy: 0.9062 - loss: 0.3699"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 65ms/step - accuracy: 0.9062 - loss: 0.3699 - val_accuracy: 1.0000 - val_loss: 0.1309\n",
            "Epoch 3/10\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 1s/step - accuracy: 0.8426 - loss: 0.4353 - val_accuracy: 0.9094 - val_loss: 0.2794\n",
            "Epoch 4/10\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 639us/step - accuracy: 0.8438 - loss: 0.4231 - val_accuracy: 1.0000 - val_loss: 0.0939\n",
            "Epoch 5/10\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 1s/step - accuracy: 0.8350 - loss: 0.4511 - val_accuracy: 0.9109 - val_loss: 0.2687\n",
            "Epoch 6/10\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - accuracy: 0.8438 - loss: 0.4480 - val_accuracy: 0.7500 - val_loss: 0.5438\n",
            "Epoch 7/10\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - accuracy: 0.8360 - loss: 0.4396 - val_accuracy: 0.9102 - val_loss: 0.2928\n",
            "Epoch 8/10\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8125 - loss: 0.3940 - val_accuracy: 0.8750 - val_loss: 0.3602\n",
            "Epoch 9/10\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 1s/step - accuracy: 0.8377 - loss: 0.4101 - val_accuracy: 0.9102 - val_loss: 0.2724\n",
            "Epoch 10/10\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - accuracy: 0.8750 - loss: 0.3499 - val_accuracy: 0.8750 - val_loss: 0.3164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 456ms/step - accuracy: 0.9180 - loss: 0.2569\n",
            "Test Accuracy: 90.99%\n"
          ]
        }
      ]
    }
  ]
}